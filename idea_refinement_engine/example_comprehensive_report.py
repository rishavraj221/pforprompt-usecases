"""
Example Comprehensive Report Generation
Demonstrates the improved report generator with realistic data
"""

import asyncio
from pipeline import IdeaValidationPipeline
from report_generator import ComprehensiveReportGenerator


async def run_comprehensive_validation_example():
    """Run a complete validation example with comprehensive reporting"""
    
    print("ğŸš€ Running Comprehensive Idea Validation Example")
    print("=" * 60)
    
    # Example idea for testing
    test_idea = "AI-powered prompt engineering assistant that helps developers debug and optimize their LLM prompts"
    
    try:
        # Initialize pipeline
        pipeline = IdeaValidationPipeline()
        
        print(f"\nğŸ“ Analyzing Idea: {test_idea}")
        print("\nğŸ”„ Running validation pipeline...")
        
        # Run validation with comprehensive reporting
        result = await pipeline.validate_idea(test_idea, save_report=True)
        
        if result["success"]:
            print(f"\nâœ… Validation completed successfully!")
            print(f"ğŸ“Š Report saved to: {result['report_filepath']}")
            print(f"â±ï¸ Analysis duration: {result['tracking']['analysis_duration_minutes']} minutes")
            print(f"ğŸ†” Validation ID: {result['tracking']['validation_id']}")
            
            # Show intermediate results
            if result.get("intermediate_results"):
                print(f"\nğŸ“‹ Intermediate Results:")
                for agent, status in result["intermediate_results"].items():
                    print(f"  â€¢ {agent}: {status}")
            
            # Show any errors
            if result.get("errors"):
                print(f"\nâš ï¸ Errors encountered:")
                for error in result["errors"]:
                    print(f"  â€¢ {error}")
            
            # Show final report preview
            if result.get("final_report"):
                print(f"\nğŸ“„ Report Preview (first 500 chars):")
                preview = result["final_report"][:500] + "..." if len(result["final_report"]) > 500 else result["final_report"]
                print(preview)
                
        else:
            print(f"\nâŒ Validation failed: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"\nğŸ’¥ Pipeline execution failed: {str(e)}")


def show_report_template_structure():
    """Show the structure of the comprehensive report template"""
    
    template_structure = """
ğŸ“‹ COMPREHENSIVE REPORT TEMPLATE STRUCTURE

ğŸš€ IDEA VALIDATION REPORT
â”œâ”€â”€ Header with tracking information
â”œâ”€â”€ Executive Summary with verdict and confidence score
â””â”€â”€ Key metrics dashboard

ğŸ¯ PROBLEM & SOLUTION ANALYSIS
â”œâ”€â”€ Problem validation with evidence
â”œâ”€â”€ Target users and pain points
â”œâ”€â”€ Solution summary and value proposition
â””â”€â”€ Solution-problem fit assessment

ğŸª MARKET LANDSCAPE
â”œâ”€â”€ Market opportunity indicators
â”œâ”€â”€ Competitive analysis
â”œâ”€â”€ Unmet needs discovery
â””â”€â”€ Competitive edge analysis

ğŸ” FEASIBILITY ASSESSMENT
â”œâ”€â”€ Technical feasibility (complexity, timeline, risks)
â”œâ”€â”€ Market feasibility (maturity, acquisition, competition)
â””â”€â”€ Operational feasibility (team, resources, scalability)

âš ï¸ CRITICAL RISKS & MITIGATION
â”œâ”€â”€ Deal-breaker risks with probability/impact
â””â”€â”€ Assumption risks with validation methods

ğŸ“‹ EXECUTION ROADMAP
â”œâ”€â”€ Phase 1: Validation (weeks 1-4)
â”œâ”€â”€ Phase 2: MVP Development (weeks 5-16)
â””â”€â”€ Phase 3: Growth (month 4+)

ğŸ”„ PIVOT OPTIONS
â”œâ”€â”€ Alternative approaches if core idea fails
â””â”€â”€ Adjacent opportunities discovered

ğŸ“Š VALIDATION DATA SOURCES
â”œâ”€â”€ Community research summary
â”œâ”€â”€ Sentiment analysis
â””â”€â”€ Expert validation sources

ğŸ¯ RECOMMENDATION & NEXT STEPS
â”œâ”€â”€ Primary recommendation (PURSUE/PIVOT/KILL)
â”œâ”€â”€ Reasoning and justification
â”œâ”€â”€ Immediate next actions
â””â”€â”€ Success criteria for next phase

ğŸ“ APPENDICES
â”œâ”€â”€ Detailed SWOT analysis
â”œâ”€â”€ Brainstorming variations
â””â”€â”€ Raw research data
"""
    
    print(template_structure)


def demonstrate_improved_report_generator():
    """Demonstrate the improved report generator with realistic data"""
    
    print("ğŸ”§ Demonstrating Improved Report Generator")
    print("=" * 50)
    
    # Create realistic sample data
    sample_state = {
        "user_idea": "AI-powered prompt engineering assistant that helps developers debug and optimize their LLM prompts",
        "clarified_idea": {
            "status": "complete",
            "core_problem": "Developers struggle to debug and optimize LLM prompts effectively",
            "proposed_solution": "AI-powered assistant that analyzes prompts and suggests improvements",
            "target_users": "Software developers, AI engineers, prompt engineers",
            "value_proposition": "Faster prompt debugging and optimization with AI-powered insights",
            "implementation_approach": "Web-based platform with API integration",
            "known_assumptions": [
                "Developers will pay for prompt optimization tools",
                "AI can effectively analyze and improve prompts",
                "Market exists for specialized prompt engineering tools"
            ]
        },
        "idea_variations": {
            "practical_variations": [
                "Browser extension for real-time prompt analysis",
                "VS Code plugin for prompt debugging",
                "API service for prompt optimization",
                "Collaborative prompt workspace",
                "Prompt template marketplace"
            ],
            "wildcard_concepts": [
                "AI that writes prompts for you based on requirements",
                "Prompt version control with git-like functionality",
                "Prompt performance analytics dashboard",
                "Cross-platform prompt sharing platform",
                "Prompt optimization as a service"
            ]
        },
        "critique_analysis": {
            "swot_analysis": {
                "strengths": [
                    "Addresses real developer pain point",
                    "AI-powered insights provide unique value",
                    "Growing market for AI development tools",
                    "Scalable SaaS business model"
                ],
                "weaknesses": [
                    "Requires significant AI/ML expertise",
                    "High development complexity",
                    "Need for large training dataset",
                    "Competition from established players"
                ],
                "opportunities": [
                    "Growing demand for AI development tools",
                    "Potential for enterprise partnerships",
                    "Expansion to other AI development areas",
                    "Integration with existing developer tools"
                ],
                "threats": [
                    "Large tech companies entering the space",
                    "Rapidly evolving AI landscape",
                    "Difficulty in maintaining competitive advantage",
                    "Potential regulatory challenges"
                ]
            },
            "feasibility_scores": {
                "technical": 6,
                "market": 7,
                "operational": 5
            },
            "assumption_risks": [
                "Market size sufficient for sustainable business: high",
                "AI can effectively analyze prompts: medium",
                "Developers will pay for this tool: medium",
                "Technical complexity manageable: high"
            ],
            "kill_risk": "low"
        },
        "reality_check": {
            "market_size_indicators": {
                "forum_mentions": 45,
                "search_volume": "High",
                "growth_trend": "Growing"
            },
            "web_research": {
                "existing_solutions": [
                    {
                        "name": "PromptPerfect",
                        "website": "https://promptperfect.com",
                        "strengths": ["User-friendly interface", "Multiple model support"],
                        "weaknesses": ["Limited customization", "High pricing"],
                        "pricing": "$29/month",
                        "user_sentiment": "positive"
                    },
                    {
                        "name": "PromptBase",
                        "website": "https://promptbase.com",
                        "strengths": ["Large prompt library", "Community features"],
                        "weaknesses": ["No optimization tools", "Quality varies"],
                        "pricing": "Free + premium",
                        "user_sentiment": "neutral"
                    }
                ],
                "forum_insights": [
                    {
                        "source": "Reddit r/MachineLearning",
                        "discussion": "Prompt engineering challenges in production",
                        "pain_points": ["Debugging complex prompts", "Optimizing for cost", "Maintaining consistency"],
                        "sentiment": "negative"
                    }
                ],
                "market_trends": [
                    "Growing demand for AI development tools",
                    "Increasing focus on prompt engineering",
                    "Rise of specialized AI tools"
                ]
            },
            "reddit_analysis": {
                "total_posts": 45,
                "subreddits_analyzed": ["MachineLearning", "OpenAI", "ArtificialIntelligence"]
            }
        },
        "user_validation_responses": [
            {
                "question": "Would you pay $20/month for an AI prompt optimization tool?",
                "response": "Yes, absolutely! I spend hours debugging prompts and would love a tool to help with this.",
                "sentiment": "positive"
            },
            {
                "question": "How often do you struggle with prompt debugging?",
                "response": "Almost daily. It's the most time-consuming part of my AI development workflow.",
                "sentiment": "negative"
            },
            {
                "question": "What's your biggest frustration with current prompt engineering tools?",
                "response": "Most tools are too basic or too expensive. Need something in between.",
                "sentiment": "neutral"
            }
        ],
        "analysis_start_time": "2025-01-27T10:00:00",
        "validation_id": "VAL_20250127_100000",
        "analysis_duration_minutes": 45
    }
    
    # Generate report using improved generator
    generator = ComprehensiveReportGenerator()
    report = generator.generate_report(sample_state, "AI-powered prompt engineering assistant")
    
    print("\nğŸ“Š Generated Report Preview:")
    print("=" * 60)
    
    # Show key sections
    sections = report.split("\n\n")
    for i, section in enumerate(sections[:5]):  # Show first 5 sections
        print(f"\n{section}")
        if i < 4:  # Don't print separator after last section
            print("-" * 40)
    
    print(f"\n... (report continues with {len(sections)-5} more sections)")
    
    # Save complete report
    with open("example_comprehensive_report.md", "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"\nâœ… Complete report saved to: example_comprehensive_report.md")
    print(f"ğŸ“ Report length: {len(report)} characters")
    
    # Analyze report quality
    print(f"\nğŸ” Report Quality Analysis:")
    
    # Check for mock data
    mock_indicators = ["[X]", "[Action 1]", "[Specific task]", "[Cost estimate]", "[X]%", "$[X]"]
    found_mock = [indicator for indicator in mock_indicators if indicator in report]
    
    if found_mock:
        print(f"âš ï¸ Found mock data indicators: {found_mock}")
    else:
        print("âœ… No mock data found - all content is data-driven")
    
    # Check metrics calculation
    if "Problem Validation|7/10" in report or "Problem Validation|8/10" in report:
        print("âœ… Problem validation score calculated from real data")
    else:
        print("âš ï¸ Problem validation score may be using default values")
    
    # Check competitive analysis
    if "PromptPerfect" in report or "PromptBase" in report:
        print("âœ… Competitive analysis includes real competitor data")
    else:
        print("âš ï¸ Competitive analysis may be generic")
    
    # Check user validation
    if "Yes, absolutely!" in report or "Almost daily" in report:
        print("âœ… User validation responses included in report")
    else:
        print("âš ï¸ User validation responses may be missing")


if __name__ == "__main__":
    print("ğŸ¯ Comprehensive Report Generation Examples")
    print("=" * 50)
    
    # Show template structure
    show_report_template_structure()
    
    print("\n" + "=" * 50)
    
    # Demonstrate improved generator
    demonstrate_improved_report_generator()
    
    print("\n" + "=" * 50)
    
    # Run full pipeline example (optional)
    print("\nğŸš€ To run a full pipeline example, uncomment the following line:")
    print("# asyncio.run(run_comprehensive_validation_example())") 